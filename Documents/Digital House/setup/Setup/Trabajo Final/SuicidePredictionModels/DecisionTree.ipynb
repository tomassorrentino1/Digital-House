{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a1d8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792dbc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Suicide_Detection.csv', engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5c2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiamos la clase a dummy y achicamos el dataset para poder entrenarlo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3783c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns = ['class'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf570979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287b1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[220000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d108957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos librerias para trabajar sobre text mining\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40555155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220000</th>\n",
       "      <td>My inability to sleep... ...It is frightening....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220001</th>\n",
       "      <td>I just woke up at 2 pm hows your day going?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220002</th>\n",
       "      <td>I want more snap streaks so I can see cute boi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220003</th>\n",
       "      <td>I read an interesting article on notesI starte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220004</th>\n",
       "      <td>The horrors of the climbing unit in Gym class ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "220000  My inability to sleep... ...It is frightening....\n",
       "220001        I just woke up at 2 pm hows your day going?\n",
       "220002  I want more snap streaks so I can see cute boi...\n",
       "220003  I read an interesting article on notesI starte...\n",
       "220004  The horrors of the climbing unit in Gym class ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining Features Matrix\n",
    "X = df.drop(['class_suicide'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e966796a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220000    0\n",
       "220001    0\n",
       "220002    0\n",
       "220003    1\n",
       "220004    0\n",
       "Name: class_suicide, dtype: uint8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Target\n",
    "y = df['class_suicide']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cbcac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training and testing sets, stratifying by class\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef3c98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a limpiar los datos de train y test\n",
    "#Usamos Tokennizer que elimine los signos de puntuación y tags html\n",
    "#Hacemos stemming para obtener las raices de las palabras en minusculas\n",
    "#Eliminamos stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1e74b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reddit(text, tokenizer, stemmer, stopwords):\n",
    "    \n",
    "    #tokens (eliminamos todos los signos de puntuación)\n",
    "    words = tokenizer.tokenize(text)\n",
    "    \n",
    "    #Stemming : raiz y minusculas:\n",
    "    stem_words = [stemmer.stem(x) for x in words]\n",
    "    \n",
    "    #eliminamos stopwords (ya pasaron por stem)\n",
    "    clean_words = [x for x in stem_words if x not in stopwords]\n",
    "    \n",
    "    result = \" \".join(clean_words)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5093a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Equipo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing for stepwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a9b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos signos de puntuacion\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "englishStemmer = SnowballStemmer(\"english\")\n",
    "stopwords_en = stopwords.words('english')\n",
    "stopwords_en_stem = [englishStemmer.stem(x) for x in stopwords_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29e373ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = [clean_reddit(x, tokenizer, englishStemmer, stopwords_en_stem) for x in X_train.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f09c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test = [clean_reddit(x, tokenizer, englishStemmer, stopwords_en_stem) for x in X_test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "636fe0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer para transformar los datos de train y test\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(clean_train)\n",
    "X_train_sparse = count_vectorizer.transform(clean_train)\n",
    "X_test_sparse = count_vectorizer.transform(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1dafca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train_sparse.todense(), \n",
    "             columns = count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc6a4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test_sparse.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45d91bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos decision tree por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf80e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3aa7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = tree.DecisionTreeClassifier(criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ede10fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_tree = model_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44851525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eff74009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8370321298443193\n",
      "[[1274  240]\n",
      " [ 252 1253]]\n"
     ]
    }
   ],
   "source": [
    "predict_tree_cat = fit_tree.predict(X_test)\n",
    "accuracy_tree = accuracy_score(y_test, predict_tree_cat)\n",
    "print(accuracy_tree)\n",
    "conf_mat_tree = confusion_matrix(y_test, predict_tree_cat)\n",
    "print(conf_mat_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1844a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84      1514\n",
      "           1       0.84      0.83      0.84      1505\n",
      "\n",
      "    accuracy                           0.84      3019\n",
      "   macro avg       0.84      0.84      0.84      3019\n",
      "weighted avg       0.84      0.84      0.84      3019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predict_tree_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec8dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
